<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Roman Castagné</title>
    <link>https://romancast.github.io/posts/</link>
    <description>Recent content in Posts on Roman Castagné</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 14 Feb 2023 00:00:00 +0200</lastBuildDate><atom:link href="https://romancast.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Artificial Pretraining of Masked Language Models</title>
      <link>https://romancast.github.io/posts/artificial_data/</link>
      <pubDate>Tue, 14 Feb 2023 00:00:00 +0200</pubDate>
      
      <guid>https://romancast.github.io/posts/artificial_data/</guid>
      <description>According to Chinchilla scaling laws for Transformers [1], data may soon be a bottleneck for training really large English language models. The vast majority of languages already lack data for training even moderately sized networks.
Thus, during the last few months, we experimented with the following question: what if we could pretrain Masked Language Models (MLM, think BERT or RoBERTa) using only artificially created data, then continue pretraining on languages with very little resources?</description>
    </item>
    
  </channel>
</rss>
